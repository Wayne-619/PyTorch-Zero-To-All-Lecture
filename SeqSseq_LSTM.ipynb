{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MorvanZhou/NLP-Tutorials\n",
        "%cd NLP-Tutorials/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxkPZUQPoVwm",
        "outputId": "2df95da6-af04-44c5-e670-4376b4aed817"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-Tutorials'...\n",
            "remote: Enumerating objects: 876, done.\u001b[K\n",
            "remote: Counting objects: 100% (167/167), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 876 (delta 136), reused 125 (delta 125), pack-reused 709\u001b[K\n",
            "Receiving objects: 100% (876/876), 917.66 KiB | 32.77 MiB/s, done.\n",
            "Resolving deltas: 100% (622/622), done.\n",
            "/content/NLP-Tutorials\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcswVXLHonRO",
        "outputId": "ab9b89f8-7d99-4af0-f394-b05828f62245"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 29.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow_addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUUTKPdffCTL",
        "outputId": "14f5bbc0-0113-4a4d-8a1f-ba0175806cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chinese time order: yy/mm/dd  ['31-04-26', '04-07-18', '33-06-06'] \n",
            "English time order: dd/M/yyyy  ['26/Apr/2031', '18/Jul/2004', '06/Jun/2033']\n",
            "vocabularies:  {'Feb', 'Dec', '8', '<GO>', '/', '<EOS>', 'Sep', '3', 'Jan', 'Jul', '<PAD>', '6', '2', 'Oct', 'Jun', '1', 'Mar', '9', '4', 'May', '7', 'Nov', 'Aug', '5', '0', 'Apr', '-'}\n",
            "x index sample: \n",
            "31-04-26\n",
            "[6 4 1 3 7 1 5 9] \n",
            "y index sample: \n",
            "<GO>26/Apr/2031<EOS>\n",
            "[14  5  9  2 15  2  5  3  6  4 13]\n",
            "t:  0 | loss: 3.300 | input:  96-06-17 | target:  17/Jun/1996 | inference:  1//JulApr<EOS>\n",
            "t:  70 | loss: 1.058 | input:  91-08-19 | target:  19/Aug/1991 | inference:  23/Feb/2020<EOS>\n",
            "t:  140 | loss: 0.821 | input:  11-04-29 | target:  29/Apr/2011 | inference:  29/Sep/2020<EOS>\n",
            "t:  210 | loss: 0.548 | input:  76-03-14 | target:  14/Mar/1976 | inference:  14/Apr/1980<EOS>\n",
            "t:  280 | loss: 0.407 | input:  83-11-19 | target:  19/Nov/1983 | inference:  19/Dec/1983<EOS>\n",
            "t:  350 | loss: 0.283 | input:  08-07-21 | target:  21/Jul/2008 | inference:  21/Jul/2008<EOS>\n",
            "t:  420 | loss: 0.230 | input:  05-02-10 | target:  10/Feb/2005 | inference:  10/Dec/2005<EOS>\n",
            "t:  490 | loss: 0.104 | input:  32-10-01 | target:  01/Oct/2032 | inference:  01/Jan/2032<EOS>\n",
            "t:  560 | loss: 0.061 | input:  81-06-29 | target:  29/Jun/1981 | inference:  29/Jun/1981<EOS>\n",
            "t:  630 | loss: 0.048 | input:  76-08-18 | target:  18/Aug/1976 | inference:  18/Aug/1976<EOS>\n",
            "t:  700 | loss: 0.027 | input:  87-02-15 | target:  15/Feb/1987 | inference:  15/Feb/1987<EOS>\n",
            "t:  770 | loss: 0.019 | input:  78-12-10 | target:  10/Dec/1978 | inference:  10/Dec/1978<EOS>\n",
            "t:  840 | loss: 0.010 | input:  80-01-15 | target:  15/Jan/1980 | inference:  15/Jan/1980<EOS>\n",
            "t:  910 | loss: 0.010 | input:  29-10-11 | target:  11/Oct/2029 | inference:  11/Oct/2029<EOS>\n",
            "t:  980 | loss: 0.006 | input:  79-06-17 | target:  17/Jun/1979 | inference:  17/Jun/1979<EOS>\n",
            "t:  1050 | loss: 0.006 | input:  02-12-22 | target:  22/Dec/2002 | inference:  22/Dec/2002<EOS>\n",
            "t:  1120 | loss: 0.004 | input:  02-05-15 | target:  15/May/2002 | inference:  15/May/2002<EOS>\n",
            "t:  1190 | loss: 0.004 | input:  15-12-29 | target:  29/Dec/2015 | inference:  29/Dec/2015<EOS>\n",
            "t:  1260 | loss: 0.004 | input:  14-11-29 | target:  29/Nov/2014 | inference:  29/Nov/2014<EOS>\n",
            "t:  1330 | loss: 0.003 | input:  89-09-25 | target:  25/Sep/1989 | inference:  25/Sep/1989<EOS>\n",
            "t:  1400 | loss: 0.003 | input:  86-10-13 | target:  13/Oct/1986 | inference:  13/Oct/1986<EOS>\n",
            "t:  1470 | loss: 0.002 | input:  18-02-07 | target:  07/Feb/2018 | inference:  07/Feb/2018<EOS>\n"
          ]
        }
      ],
      "source": [
        "# [Sequence to Sequence Learning with Neural Networks](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import utils    # this refers to utils.py in my [repo](https://github.com/MorvanZhou/NLP-Tutorials/)\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "class Seq2Seq(keras.Model):\n",
        "    def __init__(self, enc_v_dim, dec_v_dim, emb_dim, units, max_pred_len, start_token, end_token):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "        # encoder\n",
        "        self.enc_embeddings = keras.layers.Embedding(\n",
        "            input_dim=enc_v_dim, output_dim=emb_dim,  # [enc_n_vocab, emb_dim]\n",
        "            embeddings_initializer=tf.initializers.RandomNormal(0., 0.1),\n",
        "        )\n",
        "        self.encoder = keras.layers.LSTM(units=units, return_sequences=True, return_state=True)\n",
        "\n",
        "        # decoder\n",
        "        self.dec_embeddings = keras.layers.Embedding(\n",
        "            input_dim=dec_v_dim, output_dim=emb_dim,  # [dec_n_vocab, emb_dim]\n",
        "            embeddings_initializer=tf.initializers.RandomNormal(0., 0.1),\n",
        "        )\n",
        "        self.decoder_cell = keras.layers.LSTMCell(units=units)\n",
        "        decoder_dense = keras.layers.Dense(dec_v_dim)\n",
        "        # train decoder\n",
        "        self.decoder_train = tfa.seq2seq.BasicDecoder(\n",
        "            cell=self.decoder_cell,\n",
        "            sampler=tfa.seq2seq.sampler.TrainingSampler(),   # sampler for train\n",
        "            output_layer=decoder_dense\n",
        "        )\n",
        "        # predict decoder\n",
        "        self.decoder_eval = tfa.seq2seq.BasicDecoder(\n",
        "            cell=self.decoder_cell,\n",
        "            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(),       # sampler for predict\n",
        "            output_layer=decoder_dense\n",
        "        )\n",
        "\n",
        "        self.cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "        self.opt = keras.optimizers.Adam(0.01)\n",
        "        self.max_pred_len = max_pred_len\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "\n",
        "    def encode(self, x):\n",
        "        embedded = self.enc_embeddings(x)\n",
        "        init_s = [tf.zeros((x.shape[0], self.units)), tf.zeros((x.shape[0], self.units))]\n",
        "        o, h, c = self.encoder(embedded, initial_state=init_s)\n",
        "        return [h, c]\n",
        "\n",
        "    def inference(self, x):\n",
        "        s = self.encode(x)\n",
        "        done, i, s = self.decoder_eval.initialize(\n",
        "            self.dec_embeddings.variables[0],\n",
        "            start_tokens=tf.fill([x.shape[0], ], self.start_token),\n",
        "            end_token=self.end_token,\n",
        "            initial_state=s,\n",
        "        )\n",
        "        pred_id = np.zeros((x.shape[0], self.max_pred_len), dtype=np.int32)\n",
        "        for l in range(self.max_pred_len):\n",
        "            o, s, i, done = self.decoder_eval.step(\n",
        "                time=l, inputs=i, state=s, training=False)\n",
        "            pred_id[:, l] = o.sample_id\n",
        "        return pred_id\n",
        "\n",
        "    def train_logits(self, x, y, seq_len):\n",
        "        s = self.encode(x)\n",
        "        dec_in = y[:, :-1]   # ignore <EOS>\n",
        "        dec_emb_in = self.dec_embeddings(dec_in)\n",
        "        o, _, _ = self.decoder_train(dec_emb_in, s, sequence_length=seq_len)\n",
        "        logits = o.rnn_output\n",
        "        return logits\n",
        "\n",
        "    def step(self, x, y, seq_len):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = self.train_logits(x, y, seq_len)\n",
        "            dec_out = y[:, 1:]  # ignore <GO>\n",
        "            loss = self.cross_entropy(dec_out, logits)\n",
        "            grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return loss.numpy()\n",
        "\n",
        "\n",
        "def train():\n",
        "    # get and process data\n",
        "    data = utils.DateData(4000)\n",
        "    print(\"Chinese time order: yy/mm/dd \", data.date_cn[:3], \"\\nEnglish time order: dd/M/yyyy \", data.date_en[:3])\n",
        "    print(\"vocabularies: \", data.vocab)\n",
        "    print(\"x index sample: \\n{}\\n{}\".format(data.idx2str(data.x[0]), data.x[0]),\n",
        "          \"\\ny index sample: \\n{}\\n{}\".format(data.idx2str(data.y[0]), data.y[0]))\n",
        "\n",
        "    model = Seq2Seq(\n",
        "        data.num_word, data.num_word, emb_dim=16, units=32,\n",
        "        max_pred_len=11, start_token=data.start_token, end_token=data.end_token)\n",
        "\n",
        "    # training\n",
        "    for t in range(1500):\n",
        "        bx, by, decoder_len = data.sample(32)\n",
        "        loss = model.step(bx, by, decoder_len)\n",
        "        if t % 70 == 0:\n",
        "            target = data.idx2str(by[0, 1:-1])\n",
        "            pred = model.inference(bx[0:1])\n",
        "            res = data.idx2str(pred[0])\n",
        "            src = data.idx2str(bx[0])\n",
        "            print(\n",
        "                \"t: \", t,\n",
        "                \"| loss: %.3f\" % loss,\n",
        "                \"| input: \", src,\n",
        "                \"| target: \", target,\n",
        "                \"| inference: \", res,\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ]
    }
  ]
}